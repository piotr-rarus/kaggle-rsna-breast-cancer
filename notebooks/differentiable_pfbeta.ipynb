{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic F1 studies\n",
    "In this notebook we try to come up with pF1 score that will be differentiable for any input.\n",
    "The original implementation was not differentiable if both precision and recall was 0, which is the case for untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_precision, c_recall, beta = sp.symbols(\"c_precision, c_recall, beta\")\n",
    "beta_squared = beta ** 2\n",
    "# symbolic pF1 function:\n",
    "sym_pF1 = (\n",
    "    (1 + beta_squared)\n",
    "    * (c_precision * c_recall)\n",
    "    / (beta_squared * c_precision + c_recall)\n",
    ")\n",
    "sym_pF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_pF1.limit(c_precision, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_pF1.limit(c_recall, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have confirmed, that with respect to recall and precision, the pF1 score reaches 0 in the limit to (recall == 0, precision == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.plotting.plot3d(\n",
    "    sym_pF1.subs(beta, 1),\n",
    "    (c_precision, 1e-5, 1e-3),\n",
    "    (c_recall, 1e-5, 1e-3),\n",
    "    title = \"pF1 score near 0,0 w.r.t precision and recall. Visual confirmation that the pF1 goes to 0.\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_pF1.diff(c_recall).limit(c_recall, 0).limit(c_precision, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_pF1.diff(c_precision).limit(c_recall, 0).limit(c_precision, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative of pF1 is zero w.r.t precision, and `(b**2+1) / (b**2)` for recall.  \n",
    "\n",
    "If we want the pF1 to be used directly in training a DL model, the implementation of pF1 we use, should be differentiable for any expected input.\n",
    "When both precision and recall are 0 the function is undefined due to zero in the denominator.  \n",
    "\n",
    "We can hack the implementation of pF1 to pass the proper gradient if we find a function that has the same partial derivative and the same value at (precision == 0, recall == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_of_the_same_gradient_at_0 = (\n",
    "    (1 + beta_squared) / beta_squared * c_recall\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_of_the_same_gradient_at_0.diff(c_precision).limit(c_recall, 0).limit(c_precision, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_of_the_same_gradient_at_0.diff(c_recall).limit(c_recall, 0).limit(c_precision, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_of_the_same_gradient_at_0.subs(c_recall, 0).subs(c_precision, 0)  # at (0,0) the value is zero, same as pF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function we have found:\n",
    "f_of_the_same_gradient_at_0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "desired implementation of the pF1 score:\n",
    "```python\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "def pfbeta(\n",
    "    labels: NDArray[np.int_], preds: NDArray[np.float_], beta: float = 1.0\n",
    ") -> float:\n",
    "    preds = preds.clip(0, 1)\n",
    "    y_true_count = labels.sum()\n",
    "    ctp = preds[labels == 1].sum()\n",
    "    cfp = preds[labels == 0].sum()\n",
    "    beta_squared = beta * beta\n",
    "    c_precision = ctp / (ctp + cfp)\n",
    "    c_recall = ctp / y_true_count\n",
    "    if (c_precision + c_recall) == 0:\n",
    "        # by definition the return value here is 0.0\n",
    "        # returned in this way, it has the same partial derivatives \n",
    "        # w.r.t precision and recall as the true pF1 in the limit\n",
    "        # as the precision goes to 0 and the recall goes to 0\n",
    "        zero: float = (1 + beta_squared) / beta_squared * c_recall\n",
    "        return zero\n",
    "    result: float = (\n",
    "        (1 + beta_squared)\n",
    "        * (c_precision * c_recall)\n",
    "        / (beta_squared * c_precision + c_recall)\n",
    "    )\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When pF1 is implemented as above, the proper gradient is calculated. When untrained (`pF1 == 0`), the model with pF1 as a Loss will first try to increase recall, thus increase true positives. Note that for proper calculation of pF1, the pF1 should be calculated over the whole dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb8b65acad8359e73602649bec17082f8f493e8190a1c0fda505718267e15017"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
